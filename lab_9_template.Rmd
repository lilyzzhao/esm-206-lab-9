---
title: "Lab 9 - Multiple Linear Regression"
author: "Lily Zhao"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Use this template to follow along in Lab Week 9. Each code chunk you'll need is already created and named. 

**Lab 9 Objectives:**

- Explore multivariate data (SLO housing prices)
- Perform multiple linear regression
- Assess diagnostics
- Compare different models by AIC
- Explain model outputs
- Make a nice table of regression results
- Make predictions using a final model
- Visualize predictions with original data

###1. Load packages

- tidyverse
- stargazer

```{r packages, include = FALSE}

# a. Load packages 'tidyverse' and 'stargazer':

library(tidyverse)
library(stargazer)

```

###2. Load data (slo_homes.csv) as a df called 'homes', then filter to create a data frame called 'homes_sub' that only include homes in SLO, Arroyo Grande, Santa Maria-Orcutt, and Atascadero

```{r get_data, include = FALSE}

# a. Read in data as 'homes':

homes<- read_csv("slo_homes.csv")
# b. Filter to only include cities "San Luis Obispo", "Santa Maria-Orcutt", "Atascadero", and "Arroyo Grande", and call this new subset 'homes_sub':

class(homes$City)
homes_sub <- homes %>% 
  filter(City=="San Luis Obispo"| City=="Santa Maria-Orcutt"| City== "Atascadero" | City== "Arroyo Grande" )


```

###3. Go exploring (visual) + think critically about variables

*Note: It's OK to LOOK at things separately, even if you're including all in a model together!*

Example: if we want to compare distribution of housing prices by CITY (ignoring all other variables), we can do that:

```{r by_city}

# a. Calculate mean price by city
mean_by_city <- homes_sub %>% 
  group_by(City) %>% 
  summarize(
    mean = mean(Price)
  )

# b. Visualize prices by city
by_city <- ggplot(homes_sub, aes(x = Price)) +
  geom_density(aes(color = City, fill = City), alpha = 0.3) + # Note: just to show what the geom_violin shows
  theme_classic() +
  scale_x_continuous(expand = c(0,0), limits = c(0,3e6)) +
  scale_y_continuous(expand = c(0,0)) +
  labs(x = "Home Prices (USD)", y = "Density")

by_city #this is the density distribution of housing prices. bins are centered around observations. Santa-maria housing prices may be lower than other groups. means seem to match our thoughts from our density distribution

```

Or another question: Overall relationship between home square footage and price, separated by City? 

```{r by_sqft}

# a. Relationship between square footage and price
by_sqft <- ggplot(homes_sub, aes(x = SqFt, y = Price)) +
  geom_point(aes(color = City, pch = Status), alpha = 0.5) +
  facet_wrap(~Status) +
  theme_classic()

by_sqft #overall there seems to be a linear relationship. if i have categorical variables it doesn't make sense to show linearity. so don't even explore that. but if you have continous predictor variables check it out. 

# Observations here: Does relationship appear ~ linear? Anything else we can pick out re: trends, outliers, etc.? What is the general trend? Any outliers? Is there reason enough for us to omit it?

```

###4. Multiple linear regression

Multiple linear regression in R follows the same syntax we've been using so far: 

    lm(y ~ x1 + x2 + x3..., data = df_name)
    
Let's try this model a couple of different ways: 

(1) Use all available variables (saturated model) 
(2) Use only SqFt as a predictor for "home size" generally (omit Bathrooms and Bedrooms), and omit PricePerSqFt (since it's derived from two other existing variables in the model)

Use summary() to view the model output and statistics.

```{r saturated}

#describe the price of a house as it relates to the variables we have. then we are only going to use sqft as a predictor of home size. 

# a. Saturated model: 
homes_lm1<- lm(Price ~ City+ Bedrooms+ Bathrooms + SqFt + PricePerSqFt+ Status, data= homes_sub)
#collinearity = highly correlated predictor variables. 
#note this is bad because pricepersqft is calculated from the dependent variable price--- biased overfitting of model results

# b. Model summary:

summary(homes_lm1)


#how much the outcome variable will change is everything else stays the same. 

#if two houses are exactly the same in the other variables than we would expect a house in san luis to sell for 34,000 more than in arro Grande.

#p-value reflects coefficient to some extent... 
#p-value is testing the hypothesis that the intercept is zero

#this model sig predicts housing prices. 

```

The next model: Exclude price per square foot, and bedrooms/bathrooms (since these are largely explained by square footage...)

```{r subset}

# a. Updated model with variables City, SqFt, and Status:
homes_lm2 <- lm(Price ~ City+ SqFt + Status, data= homes_sub)

# b. Model summary:
summary(homes_lm2)
```

Wait...but what if I wanted everything to be with respect to a Regular sale status? Then I need to change my factor levels. We've done this before, here we'll use a different function (fct_relevel) from *forcats* package in the tidyverse. 

```{r fct_relevel}

# a. Set Status to class 'factor'
#currently lm has made it a character
homes_sub$Status<- factor(homes_sub$Status)

# b. Check to ensure it's a factor now


# c. Check levels:
levels(homes_sub$Status)

# d. Reassign reference level of "Status" to "Regular":

homes_sub$Status <- fct_relevel(homes_sub$Status, "Regular") #separate by comma 

# e. Now run the regression again - same equation, but now the reference level is different (Status = Regular): 
homes_lm3 <- lm(Price~ City  +SqFt+ Status, data = homes_sub)
summary(homes_lm3)
#doesn't change anything about your overall model fit. 
```

Interpret the coefficients and statistical outcomes above. 
Notes: 

###5. Model diagnostics

Remember, since we're concerned about *residuals* (distance that actual observations exist from model predictions), we can only evaluate some assumptions *after* running the regression. 

Then we can evaluate model diagnostics using the plot() function:

```{r diagnostics}

# a. Model diagnostics:

plot(homes_lm3)
#residual vs. leverage: is vertical scatter changing?  the only reason you would consider removing an outlier is if it is not representative of the population you are trying to study. 188 would be a good house to buy. not that worries
#is their a change is variance of residuals/residual spread. 
#red line is a weight average you should ignore. 

#qqplot don't be thrown off by a few endpoint values. pretty normally distributed. only 8/400  are not following .. assumption of normal residuals is totality satified. 

#overall heteroscedasticity is ok and residuals normality def good and conceptually and mathmatically my model is making sense!
```

###6. Model comparison by Akaike Information Criterion

The AIC is a quantitative metric for model "optimization" that balances complexity with model fit. The best models are the ones that fit the data as well as possible, as simply as possible. Recall: lower AIC value indicates a *more optimal* balance - **BUT STATISTICS IS NO SUBSTITUTE FOR JUDGEMENT!!!**

```{r AIC}

# a. AIC values for each model

AIC(homes_lm1)
AIC(homes_lm2)
AIC(homes_lm3)
#obvi lm3 and lm2 will be the same 
#absolute aic means nothing. only usefuly for comparison. 
# Answer: which would you pick? 
#this is a demonstration of why you should never select a final model based on aic. 
#i wouild still pick homes lm3. 

```

###7. Regression tables with *stargazer*

```{r stargazer, results = 'asis'}

# a. Prepare a nice regression table:

stargazer(homes_lm1, homes_lm3, type= "html")


#workaround go to the github folder. and open word 
# Note: If you want to work with this in Word, save to html, open, copy and paste into Word. 

```

###8. Making predictions

Using your final selected model, predict the housing price for a range of home sizes, sale status, and city. 

The predict() function uses the following syntax:

      predict(model_name, newdata = new_data_name)
      
Defaults are to exclude the prediction SE and mean confidence interval - if you want to include, use arguments

      se.fit = TRUE
      interval = "confidence" 
      interval = "prediction"

First, you need to create a new data frame of values that contain ALL NECESSARY VARIABLES **with the same variable names AND level strings**.

```{r df_new}

# First, make a new data frame

# Note that the df_new created below has the SAME variable names and level strings as the original model data (otherwise R won't know how to use it...)
# Work through this on your own to figure out what it actually does:

df_new <- data.frame(City = rep(c("San Luis Obispo",
                                  "Santa Maria-Orcutt",
                                  "Atascadero",
                                  "Arroyo Grande"), 
                                each = 60), 
                     SqFt = rep(seq(from = 500,
                                    to = 3500, 
                                    length = 20), 
                                times = 12), 
                     Status = rep(c("Regular",
                                    "Foreclosure",
                                    "Short Sale"), 
                                  times = 12, 
                                  each = 20))

```

Make predictions for the new data using predict():

```{r predict}

# a. Make predictions using the new data frame:

price_predict <- predict(homes_lm3, newdata= df_new, se.fit = TRUE, interval="confidence") 

#if you arennt getting a datafrmae
predict_df<- data.frame(df_new, price_predict)
#make a confidence interval about the mean value
# b. Bind predictions to the data to make it actually useful:
#confidence is 95% 
#dont worry to much about residuals. 



```

Then visualize it!

```{r graph, echo = FALSE}

# Create a line graph with square footage on the x-axis, predicted price on y-axis, with color dependent on City and facet_wrapped by sale status (follow along):

predict_graph<- ggplot(predict_df, aes(x=SqFt, y= fit.fit))+
  geom_line(aes(color= City))+
  facet_wrap(~Status)

predict_graph
```

END LAB